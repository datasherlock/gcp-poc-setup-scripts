gcloud dataproc batches submit spark \
 -—project=$(gcloud info --format='value(config.project)') \
 -—region=us-central1 \
 -—jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \
 -—class=org.apache.spark.examples.SparkPi \
 —- 1000

 gcloud beta dataproc batches submit --project hive-project-347910 --region us-central1 spark --batch batch-df03 --class org.apache.spark.examples.SparkPi 10000


  gcloud dataproc batches submit spark \
    --region=us-central1 \
    --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \
    --class=org.apache.spark.examples.SparkPi \
     --service-account=dataproc-sa@datasherlock.iam.gserviceaccount.com \
     --project="datasherlock" \
     --labels purpose=dataproc-serverless-sparkpi,arg=100000000 \
    -- 100000000

  gcloud dataproc jobs submit pyspark \
  --cluster poc-cluster-16 \
  --region us-central1 \
  --properties=spark.executor.cores=5,spark.executor.instances=8,spark.executor.memory=15g,spark.dynamicAllocation.enabled=false \
  gs://datasherlock/code/pyspark_demo.py \
  -- 10000000 gs://datasherlock/test_data

gcloud dataproc jobs submit pyspark \
  --cluster poc-cluster-16 \
  --region us-central1 \
  --properties=spark.spark.dynamicAllocation.initialExecutors=8,spark.spark.dynamicAllocation.maxExecutors=35,spark.spark.dynamicAllocation.minExecutors=8,spark.dynamicAllocation.enabled=true,spark.dynamicAllocation.executorIdleTimeout=4800s,spark.dynamicAllocation.cachedExecutorIdleTimeout=4800s \
  gs://datasherlock/code/pyspark_demo.py \
  -- 10000000 gs://datasherlock/test_data


  gcloud dataproc batches submit \
  --project datasherlock \
  --region us-central1 \
  --subnet default \
  --service-account dataproc-sa@datasherlock.iam.gserviceaccount.com \
  --properties=spark.dynamicAllocation.initialExecutors=2,spark.dynamicAllocation.maxExecutors=35,spark.dynamicAllocation.minExecutors=2,spark.dynamicAllocation.enabled=true,spark.dynamicAllocation.executorIdleTimeout=4800s,spark.dynamicAllocation.cachedExecutorIdleTimeout=4800s \
  pyspark \
  gs://datasherlock/code/pyspark_demo.py \
  -- 1000000 gs://datasherlock/test_data

gcloud dataproc jobs submit pyspark   --cluster poc-cluster-16   --region us-central1   --properties=spark.executor.cores=5,spark.executor.instances=8,spark.executor.memory=15g,spark.dynamicAllocation.enabled=false   gs://datasherlock/code/pyspark_process_agg_data.py -- gs://datasherlock/test_data/202211975917
gcloud dataproc jobs submit pyspark   --cluster poc-cluster-16   --region us-central1   --properties=spark.executor.cores=5,spark.executor.instances=8,spark.executor.memory=15g,spark.dynamicAllocation.enabled=false   gs://datasherlock/code/pyspark_process_agg_data.py -- gs://datasherlock/test_data/202211975917

gcloud dataproc jobs submit pyspark   --cluster poc-cluster-no-efm   --region us-central1   --properties=spark.executor.cores=5,spark.executor.instances=8,spark.executor.memory=15g,spark.dynamicAllocation.enabled=false   gs://datasherlock/code/pyspark_process_agg_data.py -- gs://datasherlock/test_data/202211975917
gcloud dataproc jobs submit pyspark   --cluster poc-cluster-no-efm   --region us-central1   --properties=spark.executor.cores=5,spark.executor.instances=8,spark.executor.memory=15g,spark.dynamicAllocation.enabled=false   gs://datasherlock/code/pyspark_process_agg_data.py -- gs://datasherlock/test_data/202211975917

  gcloud dataproc clusters update poc-cluster-16 --region=us-central1 --num-secondary-workers=0 
  gcloud dataproc clusters update poc-cluster-no-efm --region=us-central1 --num-secondary-workers=0
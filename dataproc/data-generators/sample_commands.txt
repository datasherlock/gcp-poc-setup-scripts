gcloud dataproc batches submit spark \
 -—project=$(gcloud info --format='value(config.project)') \
 -—region=us-central1 \
 -—jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \
 -—class=org.apache.spark.examples.SparkPi \
 —- 1000

 gcloud beta dataproc batches submit --project hive-project-347910 --region us-central1 spark --batch batch-df03 --class org.apache.spark.examples.SparkPi 10000


  gcloud dataproc batches submit spark \
    --region=us-central1 \
    --jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \
    --class=org.apache.spark.examples.SparkPi \
     --service-account=dataproc-sa@datasherlock.iam.gserviceaccount.com \
     --project="datasherlock" \
     --labels purpose=dataproc-serverless-sparkpi,arg=100000000 \
    -- 100000000

  gcloud dataproc jobs submit pyspark \
  --cluster poc-cluster-16 \
  --region us-central1 \
  --properties=spark.executor.cores=5,spark.executor.instances=8,spark.executor.memory=15g,spark.dynamicAllocation.enabled=false \
  gs://datasherlock/code/pyspark_demo.py \
  -- 10000000 gs://datasherlock/test_data

gcloud dataproc jobs submit pyspark \
  --cluster poc-cluster-16 \
  --region us-central1 \
  --properties=spark.spark.dynamicAllocation.initialExecutors=8,spark.spark.dynamicAllocation.maxExecutors=35,spark.spark.dynamicAllocation.minExecutors=8,spark.dynamicAllocation.enabled=true,spark.dynamicAllocation.executorIdleTimeout=4800s,spark.dynamicAllocation.cachedExecutorIdleTimeout=4800s \
  gs://datasherlock/code/pyspark_demo.py \
  -- 10000000 gs://datasherlock/test_data
  